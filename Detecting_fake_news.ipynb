{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrkrmAqAufn0vUCYZNLzl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mina-Rahmanian/Detecting-fake-news/blob/main/Detecting_fake_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fake or Real News Detection\n",
        "\n",
        "Install the library with pip"
      ],
      "metadata": {
        "id": "YzPruCSVT6pE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzHbuKFcC8EE"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make necessary imports:"
      ],
      "metadata": {
        "id": "kK8xcoDpUWvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools \n",
        "# itertools it is used to iterate over data structures that can be stepped over using a for-loop. That utilize computational resources efficiently.\n",
        "# It works as a fast, memory-efficient tool. Also tends to enhance the readability and maintainability of the code.\n",
        "\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Splitting data arrays into two subsets: for training data and for testing data\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Extract features in a format supported by ML algorithms from datasets consisting of formats such as text and image.\n",
        "\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix # Sklearn metrics let you assess the quality of your predictions"
      ],
      "metadata": {
        "id": "-EMAWoJi3yWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Read the data into a DataFrame, and get the shape of the data and the first 10 records."
      ],
      "metadata": {
        "id": "vrOeSRhbUix0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the data\n",
        "\n",
        "from google.colab import drive\n",
        "%matplotlib inline\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/\")\n",
        "\n",
        "# df=pd.read_csv('D:\\\\Data\\\\news.csv')\n",
        "\n",
        "#np.random.seed(7)\n",
        "df = pd.read_csv('My Drive/fakenews/news.csv')\n",
        "\n",
        "#Get shape and head\n",
        "print(df.shape)\n",
        "df.head(10) # (all dataset, featuers)"
      ],
      "metadata": {
        "id": "lDb_VsDU33zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the labels from the DataFrame."
      ],
      "metadata": {
        "id": "dhLfhx0_Uvg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Get the labels\n",
        "labels=df.label\n",
        "labels.head(10)"
      ],
      "metadata": {
        "id": "Ue3cn3d4R6g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the text from the DataFrame."
      ],
      "metadata": {
        "id": "QviCg4eHU1Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts=df.text\n",
        "texts.head(5)"
      ],
      "metadata": {
        "id": "DjUsgppM3Y8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the dataset into training and testing sets."
      ],
      "metadata": {
        "id": "kRTnB-IQU9iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.common import random_state\n",
        "#DataFlair - Split the dataset\n",
        "x_train,x_test,y_train,y_test=train_test_split(df['text'], labels, test_size=0.2, random_state=7)"
      ],
      "metadata": {
        "id": "Wrll-L-VzOan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Let’s initialize a TfidfVectorizer with stop words from the English language and a maximum document frequency of 0.7"
      ],
      "metadata": {
        "id": "YGQSXykGVFC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are the most common words in a language that are to be filtered out before processing the natural language data. And a TfidfVectorizer turns a collection of raw documents into a matrix of TF-IDF features.\n",
        "\n",
        "- Fit and transform the vectorizer on the train set, and transform the vectorizer on the test set."
      ],
      "metadata": {
        "id": "USorNyMZVOvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Initialize a TfidfVectorizer\n",
        "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "#DataFlair - Fit and transform train set, transform test set\n",
        "tfidf_train=tfidf_vectorizer.fit_transform(x_train) \n",
        "tfidf_test=tfidf_vectorizer.transform(x_test)"
      ],
      "metadata": {
        "id": "3UCJ64ZKzOm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy\n",
        "\n",
        "Initialize a PassiveAggressiveClassifier. This is. We’ll fit this on tfidf_train and y_train.\\\n",
        "Predict on the test set from the TfidfVectorizer and calculate the accuracy with accuracy_score() from sklearn.metrics."
      ],
      "metadata": {
        "id": "ZB28Li6LVTG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Initialize a PassiveAggressiveClassifier\n",
        "pac=PassiveAggressiveClassifier(max_iter=50)\n",
        "pac.fit(tfidf_train,y_train)\n",
        "#DataFlair - Predict on the test set and calculate accuracy\n",
        "y_pred=pac.predict(tfidf_test)\n",
        "score=accuracy_score(y_test,y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')"
      ],
      "metadata": {
        "id": "gUCvCVet6zhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix \n",
        "to gain insight into the number of false and true negatives and positives."
      ],
      "metadata": {
        "id": "dU7n3CH6VX4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Build confusion matrix\n",
        "confusion_matrix(y_test,y_pred, labels=['FAKE','REAL'])"
      ],
      "metadata": {
        "id": "qvEI6nac6zkc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}